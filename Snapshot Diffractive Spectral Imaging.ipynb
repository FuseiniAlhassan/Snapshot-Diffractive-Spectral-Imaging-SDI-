{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76cd1329-615c-4588-a08b-a677dd8be4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1319\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1319\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1320\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1338\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1384\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1383\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1333\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1093\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1093\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1037\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1472\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1472\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1003\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[1;32m-> 1003\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(\n\u001b[0;32m   1004\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address)\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:864\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_connection failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, exceptions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:849\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    848\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 849\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m    850\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Use CIFAR-10 for simplicity during debugging\u001b[39;00m\n\u001b[0;32m    115\u001b[0m transform_cifar \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([T\u001b[38;5;241m.\u001b[39mToTensor(), T\u001b[38;5;241m.\u001b[39mResize((N, N))])\n\u001b[1;32m--> 117\u001b[0m train_set_cifar \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform_cifar)\n\u001b[0;32m    118\u001b[0m val_set_cifar \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform_cifar)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Create data loaders with proper collate function\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:66\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m download_and_extract_archive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgz_md5)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:391\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    389\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 391\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[0;32m    393\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    394\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:121\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    118\u001b[0m     _download_file_from_remote_location(fpath, url)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# expand redirect chain if needed\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     url \u001b[38;5;241m=\u001b[39m _get_redirect_url(url, max_hops\u001b[38;5;241m=\u001b[39mmax_redirect_hops)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# check if file is located on Google Drive\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     file_id \u001b[38;5;241m=\u001b[39m _get_google_drive_file_id(url)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\utils.py:66\u001b[0m, in \u001b[0;36m_get_redirect_url\u001b[1;34m(url, max_hops)\u001b[0m\n\u001b[0;32m     63\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT}\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_hops \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39mheaders)) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m==\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:189\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, context)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:489\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    486\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    488\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 489\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    492\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:506\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    505\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 506\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    507\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:466\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    465\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 466\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1367\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1368\u001b[0m                         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1322\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1320\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1323\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Snapshot Diffractive Spectral Imaging (SDI) — Notebook (Enhanced)\n",
    "-----------------------------------------------------\n",
    "Objective\n",
    "  Simulate a snapshot diffractive spectral imager: a single diffractive optical\n",
    "  element (DOE) encodes spatial and spectral information into one 2D sensor image.\n",
    "  Implement two reconstruction approaches:\n",
    "    1) physics-based regularized inversion (matrix-free Tikhonov / CG)\n",
    "    2) data-driven reconstruction (lightweight CNN)\n",
    "\n",
    "Advanced Features Added:\n",
    "  - DOE optimization via differentiable inverse design.\n",
    "  - Integration with real hyperspectral datasets (Harvard Real-World Hyperspectral Images).\n",
    "  - Sensor noise modeling (Poisson + Gaussian) for realism.\n",
    "  - Upgraded CNN with U-Net and spectral attention.\n",
    "  - PSNR/SSIM metrics logging and CSV export.\n",
    "  - DOE height map export for fabrication.\n",
    "  - GPU support and modular structure.\n",
    "  - Real sensor image inference mode.\n",
    "  - Reconstruction evolution animations.\n",
    "\n",
    "Outputs (saved to ./results):\n",
    "  - example_sensor.png\n",
    "  - psfs.npy (point-spread functions per band)\n",
    "  - tikhonov_recon.png\n",
    "  - cnn_recon_epoch_*.png\n",
    "  - sdi_evolution.gif (animation)\n",
    "  - height_map.png\n",
    "  - metrics.csv\n",
    "  - model weights results/sdi_cnn.pth\n",
    "  - optimized_doe_phase.npy\n",
    "\n",
    "Run locally in Jupyter (CPU/GPU).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from matplotlib import animation\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "from scipy.sparse.linalg import cg\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------\n",
    "# Environment and settings\n",
    "# ----------------------------\n",
    "os.makedirs('results', exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "N = 64  # simulation grid\n",
    "physical_size = 2e-3\n",
    "dx = physical_size / N\n",
    "\n",
    "num_bands = 9\n",
    "wavelengths = torch.linspace(450e-9, 650e-9, num_bands, device=device).cpu().numpy()\n",
    "\n",
    "z = 5e-3  # propagation distance\n",
    "n_material = 1.5  # Refractive index for height map\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "lr = 1e-3\n",
    "use_hyperspec_data = False  # Set to False for now to debug with CIFAR-10 first\n",
    "noise_level = {'shot': 1e-3, 'read': 1e-4}\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset: CIFAR-10 RGB + Optional Harvard Hyperspectral\n",
    "# ----------------------------\n",
    "class HyperspecDataset(Dataset):\n",
    "    def __init__(self, root_dir='./data/hyperspec', transform=None, target_bands=num_bands):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = [f for f in os.listdir(root_dir) if f.endswith('.mat')] if os.path.exists(root_dir) else []\n",
    "        self.target_bands = target_bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mat = sio.loadmat(os.path.join(self.root_dir, self.files[idx]))\n",
    "        cube = mat.get('img', mat.get('hyper_image', np.random.rand(64, 64, 31)))  # Fallback\n",
    "        cube = cube.astype(np.float32) / cube.max()\n",
    "\n",
    "        # Resample spectral dimension if needed\n",
    "        if cube.shape[-1] != self.target_bands:\n",
    "            cube = np.interp(\n",
    "                np.linspace(0, 1, self.target_bands),\n",
    "                np.linspace(0, 1, cube.shape[-1]),\n",
    "                cube, axis=-1\n",
    "            )\n",
    "\n",
    "        # Convert to tensor and ensure correct shape\n",
    "        cube_tensor = torch.from_numpy(cube).float()\n",
    "        if cube_tensor.dim() == 3:\n",
    "            cube_tensor = cube_tensor.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
    "\n",
    "        # Resize if needed\n",
    "        if cube_tensor.shape[1] != N or cube_tensor.shape[2] != N:\n",
    "            cube_tensor = F.interpolate(cube_tensor.unsqueeze(0), size=(N, N), mode='bilinear').squeeze(0)\n",
    "\n",
    "        return cube_tensor  # Return only the cube, no label\n",
    "\n",
    "# Use CIFAR-10 for simplicity during debugging\n",
    "transform_cifar = T.Compose([T.ToTensor(), T.Resize((N, N))])\n",
    "\n",
    "train_set_cifar = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "val_set_cifar = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "\n",
    "# Create data loaders with proper collate function\n",
    "def cifar_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)  # (B, 3, H, W)\n",
    "    # Convert RGB to pseudo-hyperspectral by repeating channels\n",
    "    cubes = images.repeat(1, num_bands // 3 + (1 if num_bands % 3 > 0 else 0), 1, 1)[:, :num_bands, :, :]\n",
    "    return cubes  # Return only cubes, no labels\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_set_cifar, list(range(0, 2000)))\n",
    "val_subset = torch.utils.data.Subset(val_set_cifar, list(range(0, 400)))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=cifar_collate)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=cifar_collate)\n",
    "\n",
    "# ----------------------------\n",
    "# DOE: Trainable wavelength-dependent phase mask\n",
    "# ----------------------------\n",
    "base_phase = torch.nn.Parameter(torch.randn((N, N), device=device) * 2 * np.pi)\n",
    "optimize_doe = True\n",
    "optimizer_doe = torch.optim.Adam([base_phase], lr=0.05) if optimize_doe else None\n",
    "\n",
    "lambda_ref = wavelengths[num_bands//2]\n",
    "\n",
    "# Precompute transfer functions (Torch)\n",
    "fx = torch.fft.fftfreq(N, d=dx, device=device)\n",
    "FX, FY = torch.meshgrid(fx, fx, indexing='ij')\n",
    "Hs = []\n",
    "for wl in wavelengths:\n",
    "    k = 2 * torch.pi / wl\n",
    "    term = torch.clamp(1 - (wl * FX)**2 - (wl * FY)**2, min=0.0)\n",
    "    H = torch.exp(1j * k * z * torch.sqrt(term))\n",
    "    Hs.append(H)\n",
    "Hs = torch.stack(Hs)  # (num_bands, N, N) complex\n",
    "\n",
    "# ----------------------------\n",
    "# Forward model: simulate sensor from spectral cube (differentiable)\n",
    "# ----------------------------\n",
    "def forward_sdi(cube, base_phase, add_noise=True):\n",
    "    \"\"\"cube: (B, num_bands, N, N)\"\"\"\n",
    "    if len(cube.shape) == 3:\n",
    "        cube = cube.unsqueeze(0)\n",
    "    B = cube.shape[0]\n",
    "    amp = torch.sqrt(cube.clamp(min=1e-6))\n",
    "    phase_scale = torch.from_numpy(lambda_ref / wavelengths).float().to(device)\n",
    "    phi = base_phase[None, None, :, :] * phase_scale[None, :, None, None]\n",
    "    field = amp * torch.exp(1j * phi)\n",
    "\n",
    "    # Propagate\n",
    "    field_flat = field.view(B * num_bands, N, N)\n",
    "    H_flat = Hs.repeat(B, 1, 1).view(B * num_bands, N, N)\n",
    "    U = torch.fft.fft2(field_flat)\n",
    "    U = U * H_flat\n",
    "    u_out = torch.fft.ifft2(U)\n",
    "    intensity = torch.abs(u_out) ** 2\n",
    "    intensity = intensity.view(B, num_bands, N, N)\n",
    "    sensor = intensity.sum(dim=1)\n",
    "    sensor = sensor / (sensor.amax(dim=(1,2), keepdim=True) + 1e-12)\n",
    "\n",
    "    # Add noise\n",
    "    if add_noise:\n",
    "        shot_noise = torch.poisson(sensor / noise_level['shot']) * noise_level['shot']\n",
    "        read_noise = torch.randn_like(sensor) * noise_level['read']\n",
    "        sensor = sensor + shot_noise + read_noise\n",
    "        sensor = sensor.clamp(min=0)\n",
    "    return sensor, cube\n",
    "\n",
    "# Precompute PSFs for physics inversion\n",
    "psfs = []\n",
    "for i, wl in enumerate(wavelengths):\n",
    "    impulse = torch.zeros((1, 1, N, N), device=device)\n",
    "    impulse[0, 0, N//2, N//2] = 1.0\n",
    "    with torch.no_grad():\n",
    "        field = impulse * torch.exp(1j * base_phase.detach()[None, None, :, :] * (lambda_ref / wl))\n",
    "        U = torch.fft.fft2(field.view(1, N, N))\n",
    "        U = U * Hs[i:i+1]\n",
    "        u_out = torch.fft.ifft2(U)\n",
    "        intensity = torch.abs(u_out)**2\n",
    "        psfs.append(intensity[0,0].cpu().numpy())\n",
    "\n",
    "psfs = np.stack(psfs)\n",
    "np.save('results/psfs.npy', psfs)\n",
    "\n",
    "# ----------------------------\n",
    "# Physics-based inversion\n",
    "# ----------------------------\n",
    "def linear_forward_cube(X_cube):\n",
    "    y = np.zeros((N, N), dtype=np.float32)\n",
    "    for i in range(num_bands):\n",
    "        y += signal.fftconvolve(X_cube[i], psfs[i], mode='same')\n",
    "    return y\n",
    "\n",
    "def tikhonov_solve(y, alpha=1e-2, maxiter=40):\n",
    "    M = num_bands * N * N\n",
    "    def matvec(v):\n",
    "        V = v.reshape((num_bands, N, N))\n",
    "        Av = linear_forward_cube(V)\n",
    "        AtAv = np.zeros_like(V)\n",
    "        for i in range(num_bands):\n",
    "            AtAv[i] = signal.fftconvolve(Av, psfs[i][::-1, ::-1], mode='same')\n",
    "        return (AtAv.flatten() + alpha * v)\n",
    "    b = np.zeros((num_bands, N, N), dtype=np.float32)\n",
    "    for i in range(num_bands):\n",
    "        b[i] = signal.fftconvolve(y, psfs[i][::-1, ::-1], mode='same')\n",
    "    b = b.flatten()\n",
    "    x, info = cg(matvec, b, maxiter=maxiter)\n",
    "    return x.reshape((num_bands, N, N)).clip(0,1)\n",
    "\n",
    "# ----------------------------\n",
    "# Advanced CNN: U-Net with Spectral Attention\n",
    "# ----------------------------\n",
    "class SpectralAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // 8, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class SDINet(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=num_bands, base=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Conv2d(in_ch, base, 3, padding=1)\n",
    "        self.enc2 = nn.Conv2d(base, base*2, 3, padding=1, stride=2)\n",
    "        self.mid = nn.Conv2d(base*2, base*2, 3, padding=1)\n",
    "        self.attn = SpectralAttention(base*2)\n",
    "        self.up = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec = nn.Conv2d(base*2, base, 3, padding=1)\n",
    "        self.out = nn.Conv2d(base, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = F.relu(self.enc1(x))\n",
    "        e2 = F.relu(self.enc2(e1))\n",
    "        m = self.attn(F.relu(self.mid(e2)))\n",
    "        u = self.up(m)\n",
    "        cat = torch.cat([u, e1], dim=1)\n",
    "        d = F.relu(self.dec(cat))\n",
    "        return torch.sigmoid(self.out(d))\n",
    "\n",
    "model = SDINet().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if optimize_doe:\n",
    "    opt.add_param_group({'params': [base_phase]})\n",
    "\n",
    "# ----------------------------\n",
    "# Training loop\n",
    "# ----------------------------\n",
    "sample_snapshots = []\n",
    "metrics_log = {'epoch': [], 'psnr_avg': [], 'ssim_avg': []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    if optimize_doe:\n",
    "        base_phase.requires_grad_(True)\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    for cubes in pbar:  # cubes is directly from collate_fn\n",
    "        cubes = cubes.to(device)\n",
    "        sensors, _ = forward_sdi(cubes, base_phase)\n",
    "        inp = sensors.unsqueeze(1)\n",
    "        pred = model(inp)\n",
    "        loss = criterion(pred, cubes)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    print(f'Epoch {epoch+1} Train Loss: {train_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    if optimize_doe:\n",
    "        base_phase.requires_grad_(False)\n",
    "    val_psnr, val_ssim = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cubes in val_loader:  # cubes is directly from collate_fn\n",
    "            cubes = cubes.to(device)\n",
    "            sensors, _ = forward_sdi(cubes, base_phase, add_noise=False)\n",
    "            inp = sensors.unsqueeze(1)\n",
    "            pred = model(inp).cpu().numpy()\n",
    "            cubes_np = cubes.cpu().numpy()\n",
    "\n",
    "            for b in range(pred.shape[0]):\n",
    "                for band in range(num_bands):\n",
    "                    val_psnr.append(psnr(cubes_np[b, band], pred[b, band], data_range=1.0))\n",
    "                    val_ssim.append(ssim(cubes_np[b, band], pred[b, band], data_range=1.0))\n",
    "\n",
    "    avg_psnr = np.mean(val_psnr)\n",
    "    avg_ssim = np.mean(val_ssim)\n",
    "    metrics_log['epoch'].append(epoch+1)\n",
    "    metrics_log['psnr_avg'].append(avg_psnr)\n",
    "    metrics_log['ssim_avg'].append(avg_ssim)\n",
    "    print(f'Epoch {epoch+1} Val PSNR: {avg_psnr:.2f} dB, SSIM: {avg_ssim:.4f}')\n",
    "\n",
    "    # Snapshot for animation\n",
    "    with torch.no_grad():\n",
    "        sample_cube = cubes[0:1]  # First validation batch\n",
    "        sample_sensor, _ = forward_sdi(sample_cube, base_phase)\n",
    "        pred = model(sample_sensor.unsqueeze(1))[0].cpu().numpy()\n",
    "        stacked = np.concatenate([\n",
    "            sample_sensor[0].cpu().numpy()[None, ...],\n",
    "            pred,\n",
    "            sample_cube[0].cpu().numpy()\n",
    "        ], axis=0)\n",
    "        sample_snapshots.append(stacked)\n",
    "\n",
    "# Save metrics\n",
    "pd.DataFrame(metrics_log).to_csv('results/metrics.csv', index=False)\n",
    "\n",
    "# Save optimized DOE and height map\n",
    "def to_np(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "optimized_phase = to_np(base_phase)\n",
    "np.save('results/optimized_doe_phase.npy', optimized_phase)\n",
    "height = (optimized_phase % (2*np.pi)) / (2*np.pi / lambda_ref * (n_material - 1)) * 1e6\n",
    "plt.imshow(height, cmap='viridis')\n",
    "plt.colorbar(label='Height (um)')\n",
    "plt.savefig('results/height_map.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'results/sdi_cnn.pth')\n",
    "\n",
    "# ----------------------------\n",
    "# Animations\n",
    "# ----------------------------\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "frames = []\n",
    "for snap in sample_snapshots:\n",
    "    grid = np.zeros((3*N, num_bands*N))\n",
    "    sensor = snap[0]\n",
    "    pred = snap[1:num_bands+1]\n",
    "    true = snap[num_bands+1:]\n",
    "    for i in range(num_bands):\n",
    "        grid[0:N, i*N:(i+1)*N] = true[i]\n",
    "        grid[N:2*N, i*N:(i+1)*N] = pred[i]\n",
    "    grid[2*N:3*N, 0:N] = sensor\n",
    "    im = plt.imshow(grid, cmap='gray', animated=True)\n",
    "    plt.axis('off')\n",
    "    frames.append([im])\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=800, blit=True)\n",
    "ani.save('results/sdi_evolution.gif', writer='pillow')\n",
    "plt.close()\n",
    "\n",
    "print('Enhanced SDI notebook complete. Results saved in ./results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f9ded-124a-4bcb-ba58-edd1f7bc27c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
